{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064f142c",
   "metadata": {},
   "source": [
    "Text pre-processing is one of mandatory steps we will preform while creating a NLP application. As humans, the text we usually write contains lots of spelling errors, short words, special symbols, emojis, etc, which we can understand but we need to preprocess this text if we want the computer to understand it. In this notebook, we will discuss some of the types of text pre-processing you will need to perform while working with text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31470db8",
   "metadata": {},
   "source": [
    "#### Table of Contents\n",
    "\n",
    "1. Lowercasing\n",
    "2. Removing HTML Tags\n",
    "3. Removing URLs\n",
    "4. Removing Punctuations\n",
    "5. Chat word treatment\n",
    "6. Spelling Correction\n",
    "7. Removing stop words\n",
    "8. Handling Emojis\n",
    "9. Tokenization\n",
    "10. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a128c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "imdb_reviews = pd.read_csv('dataset/IMDB Dataset.csv')\n",
    "twitter_tweets = pd.read_csv('dataset/twitter_sentiment_analysis.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f38485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53e7393e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b72dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "978480aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58ee6e8",
   "metadata": {},
   "source": [
    "### Lowercasing\n",
    "\n",
    "It is the process of converting a word into lower case. If a particular word (lets say Book) appears in the beginning of the sentence with a capital letter and another word (book) appears later in the sentence without a capital letter, our model will treat these 2 words differently. The process of lowercasing is usually very simple, we can use the .lower() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7daf1795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lowercase(column):\n",
    "    column = column.str.lower()\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53a63c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before applying lower casing: One of the\n",
      "After applying lower casing : one of the\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before applying lower casing: {imdb_reviews['review'][0][:10]}\")\n",
    "\n",
    "imdb_reviews['review'] = convert_lowercase(imdb_reviews['review'])\n",
    "\n",
    "print(f\"After applying lower casing : {imdb_reviews['review'][0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45013c46",
   "metadata": {},
   "source": [
    "### Removing HTML Tags\n",
    "\n",
    "Whenever you will scrape a website, HTML tags such as header, body, anchor, etc, will be present. These tags won't add any value to the text data we have and therefore, should be removed. We can remove these HTML tags by using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b2cf6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tags(text):\n",
    "    re_html = re.compile('<.*?>')\n",
    "    return re_html.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d77b737d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is a h1 tag \n"
     ]
    }
   ],
   "source": [
    "text = '<h1> This is a h1 tag </h1>'\n",
    "print(remove_html_tags(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f218a02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing HTML tags: a wonderful little production. <br /><br />the filming technique is ve\n",
      "After removing HTML tags : a wonderful little production. the filming technique is very unassumin\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before removing HTML tags: {imdb_reviews['review'][1][:70]}\")\n",
    "imdb_reviews['review'] = imdb_reviews['review'].apply(remove_html_tags)\n",
    "print(f\"After removing HTML tags : {imdb_reviews['review'][1][:70]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0db6dc",
   "metadata": {},
   "source": [
    "### Removing URLs\n",
    "\n",
    "URLs in a text references to a location to the web but just like HTML tags, it doesn't provide any useful information. We can remove URLs by using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "955ac646",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'My profile link: https://www.kaggle.com/anubhavgoyal10'\n",
    "def remove_url(text):\n",
    "    re_url = re.compile('https?://\\S+|www\\.\\S+')\n",
    "    return re_url.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8def667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text before removing URL: My profile link: https://www.kaggle.com/anubhavgoyal10\n",
      "Text after removing URL : My profile link: \n"
     ]
    }
   ],
   "source": [
    "print(f'Text before removing URL: {text}')\n",
    "print(f'Text after removing URL : {remove_url(text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a126f2",
   "metadata": {},
   "source": [
    "### Removing Puncuations\n",
    "\n",
    "The reason of removing punctuations is pretty simiar to lowercasing, in certain cases, we want the word hello and hello! to be treated in the exact same way. Although, be careful while using punctuation, the word can't can be converted to cant and can t depending upon what you set in the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f777be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "exclude = string.punctuation\n",
    "\n",
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('', '', exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d40bc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text before punctuation: Hello!\n",
      "Text after punctuation : Hello\n"
     ]
    }
   ],
   "source": [
    "text = 'Hello!'\n",
    "print(f'Text before punctuation: {text}')\n",
    "text_wihout_punc = remove_punc(text)\n",
    "print(f'Text after punctuation : {text_wihout_punc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3827548e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet before removing punctuation:  @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run\n",
      "Tweet after removing punctuation :  user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction   run\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tweet before removing punctuation: {twitter_tweets['tweet'][0]}\")\n",
    "twitter_tweets['tweet'] = twitter_tweets['tweet'].apply(remove_punc)\n",
    "print(f\"Tweet after removing punctuation : {twitter_tweets['tweet'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c19d43a",
   "metadata": {},
   "source": [
    "### Chat Word Treatment\n",
    "\n",
    "Many times we use short abbreviations of words while we are texting. We have to change them back into their full forms while performing NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eda00f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some chat words examples\n",
    "chat_words = {\n",
    "    'FYI' : 'for your information',\n",
    "    'LOL' : 'laugh out loud',\n",
    "    'AFK' : 'away from keyboard'\n",
    "}\n",
    "\n",
    "def chat_words_conv(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word.upper() in chat_words:\n",
    "            new_text.append(chat_words[word.upper()])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31134bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for your information I was away from keyboard for a while\n"
     ]
    }
   ],
   "source": [
    "text = 'FyI I was afk for a while'\n",
    "print(chat_words_conv(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830dc9dc",
   "metadata": {},
   "source": [
    "### Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8366daa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct text: string with lots of spelling errors\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "text = 'stringg witth lotts of spelingg erors'\n",
    "textblob_ = TextBlob(text)\n",
    "print(f'Correct text: {textblob_.correct().string}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc8e18",
   "metadata": {},
   "source": [
    "### Removing stop Words\n",
    "\n",
    "Stop words are words such as the, an, so, and which are present in abundance in every text but they don't provide much uselful information to the model, so by removing these words, we can focus on the more important information in the text. Although in some cases, we don't remove these stop words, one example being sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ce3c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_english = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in stopwords_english:\n",
    "            continue\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72b47fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text before removing stop words: Stop words are a set of commonly used words in a language\n",
      "Text after removing stop words : Stop words set commonly used words language\n"
     ]
    }
   ],
   "source": [
    "text = 'Stop words are a set of commonly used words in a language'\n",
    "print(f'Text before removing stop words: {text}')\n",
    "print(f'Text after removing stop words : {remove_stopwords(text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3dfe3b",
   "metadata": {},
   "source": [
    "### Handling Emojis\n",
    "\n",
    "Emojis are generally used in a text to express emotions. There are 2 ways in which we can handle emojis, either we can just remove them (not a good option as they can provide some useful information), and the second good will be to replace them in a way that computer can understand, we can do that using the emoji library in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4334286c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is suffering from fever :face_with_thermometer:\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "text = 'He is suffering from fever 🤒'\n",
    "print(emoji.demojize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45940402",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "It is the process of breaking the data into smaller chunks of information. We can use tokenization to seperate sentences, words, characters. We can perform tokenization using the nltk or spacy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "133be738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Life', 'is', 'a', 'matter', 'of', 'choices', 'and', 'every', 'choice', 'makes', 'you', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "sent_1 = 'Life is a matter of choices and every choice makes you!'\n",
    "print(word_tokenize(sent_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e102b",
   "metadata": {},
   "source": [
    "As we can see, it managed to separate the you and !. If you used the Python split function, it wouldn't have done that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af4d6c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't forget that gifts often come with costs that go beyond their purchase price.\", \"When you purchase a child the latest smartphone, you're also committing to a monthly phone bill.\", \"When you purchase the latest gaming system, you're likely not going to be satisfied with the games that come with it for long and want to purchase new titles to play.\", \"When you buy gifts it's important to remember that some come with additional costs down the road that can be much more expensive than the initial gift itself.\"]\n"
     ]
    }
   ],
   "source": [
    "para = \"Don't forget that gifts often come with costs that go beyond their purchase price. When you purchase a child the latest smartphone, you're also committing to a monthly phone bill. When you purchase the latest gaming system, you're likely not going to be satisfied with the games that come with it for long and want to purchase new titles to play. When you buy gifts it's important to remember that some come with additional costs down the road that can be much more expensive than the initial gift itself.\"\n",
    "print(sent_tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "637a8332",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Plain typing.NoReturn is not valid as type argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_170412/760038957.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msent_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'A 5km bike ride costs around $10 in New York!'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\datascience\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minfo\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mglossary\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexplain\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\datascience\\lib\\site-packages\\spacy\\pipeline\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mattributeruler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAttributeRuler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdep_parser\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDependencyParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0medit_tree_lemmatizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEditTreeLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mentity_linker\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEntityLinker\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mner\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEntityRecognizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\datascience\\lib\\site-packages\\spacy\\pipeline\\attributeruler.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpipe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mErrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExample\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\datascience\\lib\\site-packages\\spacy\\pipeline\\pipe.pyx\u001b[0m in \u001b[0;36minit spacy.pipeline.pipe\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\datascience\\lib\\site-packages\\spacy\\strings.pyx\u001b[0m in \u001b[0;36minit spacy.strings\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\datascience\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   1645\u001b[0m     \u001b[0mproc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Pipe\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1646\u001b[0m     \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1647\u001b[1;33m     \u001b[0mdefault_error_handler\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Pipe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Doc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1648\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1649\u001b[0m ) -> Iterator[\"Doc\"]:\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\datascience\\lib\\typing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    753\u001b[0m                                 f\" Got {args}\")\n\u001b[0;32m    754\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem_inner__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0m_tp_cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\datascience\\lib\\typing.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[1;32mpass\u001b[0m  \u001b[1;31m# All real errors (not unhashable args) are raised below.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\datascience\\lib\\typing.py\u001b[0m in \u001b[0;36m__getitem_inner__\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    772\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Callable[args, result]: result must be a type.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 774\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_type_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    775\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mEllipsis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_TypingEllipsis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\datascience\\lib\\typing.py\u001b[0m in \u001b[0;36m_type_check\u001b[1;34m(arg, msg, is_argument)\u001b[0m\n\u001b[0;32m    133\u001b[0m     if (isinstance(arg, _SpecialForm) and arg is not Any or\n\u001b[0;32m    134\u001b[0m             arg in (Generic, _Protocol)):\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Plain {arg} is not valid as type argument\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Plain typing.NoReturn is not valid as type argument"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sent_2 = 'A 5km bike ride costs around $10 in New York!'\n",
    "doc = nlp(sent_2)\n",
    "for token in doc:\n",
    "    print(token, end= ', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f233e37",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is a process by which we bring the words to their root forms. For e.g. the stem of walking, walks, walked is walk. We can do stemming using the nltk libray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3b980f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def perform_stemming(text):\n",
    "    new_text = [ps.stem(word) for word in text.split()]\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69e4b5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'walk walks walked walking'\n",
    "perform_stemming(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a8e058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
